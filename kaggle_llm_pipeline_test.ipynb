{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3397e82c",
   "metadata": {},
   "source": [
    "# Kaggle Test Notebook: LLM Agent Pipeline (Pre-WAN)\n",
    "\n",
    "This notebook validates that the LLM-only agent pipeline is working before feeding prompts into WAN video generation.\n",
    "\n",
    "It will:\n",
    "1. Configure OpenRouter credentials safely\n",
    "2. Call the LLM directly for a seed prompt smoke test\n",
    "3. Run `llm_pipeline.py`\n",
    "4. Validate generated artifacts\n",
    "5. Export results in an embedded (vectorized) form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387b594",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install openai requests scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13214746",
   "metadata": {},
   "source": [
    "## 2) Clone repository (if needed) and set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d712d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "WORK_DIR = Path(\"/kaggle/working\")\n",
    "CLONE_DIR = WORK_DIR / \"tonmoy99_create-RL-Brain\"\n",
    "\n",
    "if not CLONE_DIR.exists():\n",
    "    !git clone https://github.com/Tonmoy221/tonmoy99_create-RL-Brain.git \"{CLONE_DIR.name}\"\n",
    "\n",
    "search_roots = [\n",
    "    WORK_DIR / \"tonmoy99_Vedio-Gen\",\n",
    "    CLONE_DIR,\n",
    "]\n",
    "\n",
    "matches = []\n",
    "for root in search_roots:\n",
    "    if root.exists():\n",
    "        matches.extend(root.rglob(\"llm_pipeline.py\"))\n",
    "\n",
    "if not matches:\n",
    "    raise FileNotFoundError(\n",
    "        \"llm_pipeline.py not found under /kaggle/working. \"\n",
    "        \"Expected in your project folder after clone.\"\n",
    "    )\n",
    "\n",
    "# Prefer the intended project folder if multiple matches exist.\n",
    "matches = sorted(\n",
    "    matches,\n",
    "    key=lambda p: (\n",
    "        \"tonmoy99_Vedio-Gen\" not in str(p.parent),\n",
    "        len(str(p.parent)),\n",
    "    ),\n",
    ")\n",
    "\n",
    "script_path = matches[0]\n",
    "repo_path = script_path.parent\n",
    "\n",
    "os.chdir(repo_path)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Using llm_pipeline.py:\", script_path)\n",
    "print(\"llm_pipeline.py exists:\", Path(\"llm_pipeline.py\").exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114b6e1",
   "metadata": {},
   "source": [
    "## 3) Configure API credentials (safe way)\n",
    "Use Kaggle Secrets for `OPENROUTER_API_KEY` instead of hardcoding keys in notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secrets = UserSecretsClient()\n",
    "OPENROUTER_API_KEY = secrets.get_secret(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise RuntimeError(\"Missing Kaggle secret OPENROUTER_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENROUTER_API_KEY\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\"\n",
    "os.environ[\"OPENAI_HTTP_REFERER\"] = \"https://www.kaggle.com\"\n",
    "os.environ[\"OPENAI_X_TITLE\"] = \"LLM Agent Pipeline Test\"\n",
    "\n",
    "print(\"Environment configured for OpenRouter through OpenAI-compatible client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584870b",
   "metadata": {},
   "source": [
    "## 4) Direct LLM smoke test (OpenRouter)\n",
    "This verifies API/model is reachable and generates a cinematic seed prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58add764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "smoke_client = OpenAI(\n",
    "    base_url=os.environ[\"OPENAI_BASE_URL\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "smoke_completion = smoke_client.chat.completions.create(\n",
    "    extra_headers={\n",
    "        \"HTTP-Referer\": os.environ.get(\"OPENAI_HTTP_REFERER\", \"https://www.kaggle.com\"),\n",
    "        \"X-Title\": os.environ.get(\"OPENAI_X_TITLE\", \"LLM Agent Pipeline Test\"),\n",
    "    },\n",
    "    model=\"google/gemma-3n-e4b-it:free\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Generate one cinematic seed prompt for a 4-scene short film with strong character and location continuity.\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "seed_prompt = smoke_completion.choices[0].message.content.strip()\n",
    "print(\"Generated seed prompt:\\n\")\n",
    "print(seed_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8118ded",
   "metadata": {},
   "source": [
    "## 5) Run the LLM Agent Pipeline (no WAN)\n",
    "Runs your standalone `llm_pipeline.py` and writes artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d71fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "script_path = repo_root / \"llm_pipeline.py\"\n",
    "if not script_path.exists():\n",
    "    raise FileNotFoundError(f\"Cannot run pipeline: missing {script_path}\")\n",
    "\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "try:\n",
    "    import llm_pipeline as lp\n",
    "    from openai import OpenAI\n",
    "\n",
    "    def _openai_compat_call(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\", \"\").strip()\n",
    "        if not api_key:\n",
    "            raise RuntimeError(\"Missing OPENAI_API_KEY environment variable\")\n",
    "\n",
    "        base_url = os.getenv(\"OPENAI_BASE_URL\", \"\").strip() or None\n",
    "        client_kwargs = {\"api_key\": api_key}\n",
    "        if base_url is not None:\n",
    "            client_kwargs[\"base_url\"] = base_url\n",
    "\n",
    "        client = OpenAI(**client_kwargs)\n",
    "\n",
    "        extra_headers = {}\n",
    "        http_referer = os.getenv(\"OPENAI_HTTP_REFERER\", \"\").strip()\n",
    "        x_title = os.getenv(\"OPENAI_X_TITLE\", \"\").strip()\n",
    "        if http_referer:\n",
    "            extra_headers[\"HTTP-Referer\"] = http_referer\n",
    "        if x_title:\n",
    "            extra_headers[\"X-Title\"] = x_title\n",
    "\n",
    "        # Try normal system+user first; fallback to user-only for models/providers\n",
    "        # that reject developer/system instructions (e.g. some free Gemma endpoints).\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                temperature=0.4,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ],\n",
    "                extra_headers=extra_headers if extra_headers else None,\n",
    "            )\n",
    "            return response.choices[0].message.content or \"\"\n",
    "        except Exception as first_exc:\n",
    "            text = str(first_exc).lower()\n",
    "            needs_fallback = \"developer instruction is not enabled\" in text\n",
    "            if not needs_fallback:\n",
    "                raise\n",
    "\n",
    "            merged_user_prompt = (\n",
    "                \"Follow these instructions exactly:\\n\"\n",
    "                f\"{system_prompt}\\n\\n\"\n",
    "                \"User request:\\n\"\n",
    "                f\"{user_prompt}\"\n",
    "            )\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                temperature=0.4,\n",
    "                messages=[{\"role\": \"user\", \"content\": merged_user_prompt}],\n",
    "                extra_headers=extra_headers if extra_headers else None,\n",
    "            )\n",
    "            return response.choices[0].message.content or \"\"\n",
    "\n",
    "    # Runtime monkey patch for compatibility in Kaggle clone.\n",
    "    lp.LLMDirectorAgent._call_openai = _openai_compat_call\n",
    "\n",
    "    report_out = lp.run_llm_pipeline(\n",
    "        seed_prompt=seed_prompt,\n",
    "        output_root=\".\",\n",
    "        provider=\"openai\",\n",
    "        model_name=\"google/gemma-3n-e4b-it:free\",\n",
    "        resume=False,\n",
    "    )\n",
    "    print(\"Pipeline completed successfully.\")\n",
    "    print(\"Report path:\", report_out)\n",
    "except Exception as exc:\n",
    "    print(\"Pipeline failed with exception:\", str(exc))\n",
    "    print(\"Detailed traceback:\\n\")\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d97ae",
   "metadata": {},
   "source": [
    "## 6) Validate pipeline artifacts and pass/fail status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "creative_doc_path = Path(\"story_bible/llm_only/creative_document_llm.json\")\n",
    "scene_prompts_path = Path(\"output/llm_only/scene_prompts_llm.json\")\n",
    "report_path = Path(\"output/llm_only/llm_pipeline_report.json\")\n",
    "memory_path = Path(\"memory_llm/state_llm.json\")\n",
    "\n",
    "required_paths = [creative_doc_path, scene_prompts_path, report_path, memory_path]\n",
    "missing = [str(p) for p in required_paths if not p.exists()]\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing expected artifacts: {missing}\")\n",
    "\n",
    "creative_doc = json.loads(creative_doc_path.read_text(encoding=\"utf-8\"))\n",
    "scene_prompts = json.loads(scene_prompts_path.read_text(encoding=\"utf-8\"))\n",
    "report = json.loads(report_path.read_text(encoding=\"utf-8\"))\n",
    "memory_state = json.loads(memory_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "scene_count = len(creative_doc.get(\"scenes\", []))\n",
    "mean_score = float(report.get(\"mean_critique_score\", 0.0))\n",
    "continuity_count = len(memory_state.get(\"continuity_log\", []))\n",
    "\n",
    "print(\"PASS: LLM pipeline artifacts generated\")\n",
    "print(\"scene_count:\", scene_count)\n",
    "print(\"mean_critique_score:\", round(mean_score, 4))\n",
    "print(\"continuity_log_count:\", continuity_count)\n",
    "print(\"report:\", report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14f398",
   "metadata": {},
   "source": [
    "## 7) Embedded result export (vector form)\n",
    "Convert generated prompts into numeric vectors and save as artifact before WAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = [item.get(\"prompt\", \"\") for item in scene_prompts if item.get(\"prompt\")]\n",
    "if not texts:\n",
    "    raise ValueError(\"No prompts found in scene_prompts_llm.json\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=64)\n",
    "embeddings = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "embedded_rows = []\n",
    "for idx, row in enumerate(scene_prompts, start=1):\n",
    "    vec = embeddings[idx - 1].tolist() if idx - 1 < len(embeddings) else []\n",
    "    embedded_rows.append(\n",
    "        {\n",
    "            \"scene_id\": row.get(\"scene_id\"),\n",
    "            \"prompt\": row.get(\"prompt\", \"\"),\n",
    "            \"critique_score\": row.get(\"critique\", {}).get(\"score\", 0.0),\n",
    "            \"embedding\": vec,\n",
    "        }\n",
    "    )\n",
    "\n",
    "embedded_path = Path(\"output/llm_only/scene_prompts_embedded.json\")\n",
    "embedded_path.write_text(json.dumps(embedded_rows, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "preview = pd.DataFrame(\n",
    "    {\n",
    "        \"scene_id\": [r[\"scene_id\"] for r in embedded_rows],\n",
    "        \"critique_score\": [r[\"critique_score\"] for r in embedded_rows],\n",
    "        \"embedding_dim\": [len(r[\"embedding\"]) for r in embedded_rows],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Embedded artifact saved:\", embedded_path)\n",
    "display(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a71b31",
   "metadata": {},
   "source": [
    "## 8) Ready-for-WAN checklist\n",
    "If all checks pass, use `output/llm_only/scene_prompts_llm.json` or `output/llm_only/scene_prompts_embedded.json` as input planning artifacts for WAN generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
