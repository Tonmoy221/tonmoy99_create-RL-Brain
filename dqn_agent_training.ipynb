{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d41dfd",
   "metadata": {},
   "source": [
    "# DQN Reinforcement Learning Notebook\n",
    "This notebook builds, trains, evaluates, and saves a DQN-style reinforcement learning agent in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe11da",
   "metadata": {},
   "source": [
    "## 1) Install and Import Dependencies\n",
    "Install and import required packages for environment simulation, training, and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9638b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed in a fresh environment, uncomment the next line:\n",
    "# !pip install gymnasium torch numpy matplotlib\n",
    "\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788c020",
   "metadata": {},
   "source": [
    "## 2) Configure Environment and Hyperparameters\n",
    "Set random seeds, device, and DQN training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_CAPACITY = 50_000\n",
    "MIN_REPLAY_SIZE = 1_000\n",
    "TARGET_UPDATE_EVERY = 200\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "NUM_EPISODES = 250\n",
    "MAX_STEPS_PER_EPISODE = 1000\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ecb904",
   "metadata": {},
   "source": [
    "## 3) Create the RL Environment Wrapper\n",
    "Initialize the environment and define helper functions for reset/step and episode stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1542e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"state_dim:\", state_dim, \"action_dim:\", action_dim)\n",
    "\n",
    "\n",
    "def env_reset(environment, seed=None):\n",
    "    state, info = environment.reset(seed=seed)\n",
    "    return np.array(state, dtype=np.float32), info\n",
    "\n",
    "\n",
    "def env_step(environment, action):\n",
    "    next_state, reward, terminated, truncated, info = environment.step(action)\n",
    "    done = terminated or truncated\n",
    "    return np.array(next_state, dtype=np.float32), float(reward), done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad6afa",
   "metadata": {},
   "source": [
    "## 4) Implement Replay Buffer\n",
    "Store transitions and sample randomized mini-batches for stable off-policy learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append(\n",
    "            Transition(\n",
    "                np.array(state, dtype=np.float32),\n",
    "                int(action),\n",
    "                float(reward),\n",
    "                np.array(next_state, dtype=np.float32),\n",
    "                float(done),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = torch.tensor(np.array([t.state for t in batch]), dtype=torch.float32, device=DEVICE)\n",
    "        actions = torch.tensor([[t.action] for t in batch], dtype=torch.int64, device=DEVICE)\n",
    "        rewards = torch.tensor([[t.reward] for t in batch], dtype=torch.float32, device=DEVICE)\n",
    "        next_states = torch.tensor(np.array([t.next_state for t in batch]), dtype=torch.float32, device=DEVICE)\n",
    "        dones = torch.tensor([[t.done] for t in batch], dtype=torch.float32, device=DEVICE)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "\n",
    "replay_buffer = ReplayBuffer(REPLAY_CAPACITY)\n",
    "print(\"Replay buffer initialized with capacity:\", REPLAY_CAPACITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e0e7f",
   "metadata": {},
   "source": [
    "## 5) Build the Q-Network (PyTorch)\n",
    "Define the Q-network and create policy/target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6352cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "policy_net = QNetwork(state_dim, action_dim).to(DEVICE)\n",
    "target_net = QNetwork(state_dim, action_dim).to(DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "print(\"Networks initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bf250",
   "metadata": {},
   "source": [
    "## 6) Implement Action Selection (Epsilon-Greedy)\n",
    "Choose random actions with probability epsilon, otherwise greedy actions from Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b525a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: np.ndarray, epsilon: float) -> int:\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    state_t = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_vals = policy_net(state_t)\n",
    "    return int(torch.argmax(q_vals, dim=1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c71f73",
   "metadata": {},
   "source": [
    "## 7) Implement Optimization Step and Target Network Sync\n",
    "Compute TD targets, optimize policy network, clip gradients, and periodically sync target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(replay_buffer) < max(BATCH_SIZE, MIN_REPLAY_SIZE):\n",
    "        return None\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states).max(1, keepdim=True)[0]\n",
    "        targets = rewards + (1.0 - dones) * GAMMA * next_q_values\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(q_values, targets)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=10.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss.item())\n",
    "\n",
    "\n",
    "def maybe_sync_target(step_idx: int):\n",
    "    if step_idx % TARGET_UPDATE_EVERY == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571870b",
   "metadata": {},
   "source": [
    "## 8) Run the Training Loop\n",
    "Train the agent across episodes with epsilon decay, replay sampling, and reward/loss logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e4632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "loss_history = []\n",
    "epsilon = EPS_START\n",
    "\n",
    "global_step = 0\n",
    "for episode in range(1, NUM_EPISODES + 1):\n",
    "    state, _ = env_reset(env, seed=SEED + episode)\n",
    "    episode_reward = 0.0\n",
    "\n",
    "    for _ in range(MAX_STEPS_PER_EPISODE):\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env_step(env, action)\n",
    "\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        loss = optimize_model()\n",
    "        if loss is not None:\n",
    "            loss_history.append(loss)\n",
    "\n",
    "        global_step += 1\n",
    "        maybe_sync_target(global_step)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    epsilon = max(EPS_END, epsilon * EPS_DECAY)\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        recent_mean = float(np.mean(episode_rewards[-20:]))\n",
    "        print(f\"Episode {episode:4d} | epsilon={epsilon:.3f} | mean reward (last 20)={recent_mean:.2f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(episode_rewards, label=\"Episode reward\")\n",
    "if len(episode_rewards) >= 20:\n",
    "    mov_avg = np.convolve(episode_rewards, np.ones(20) / 20, mode=\"valid\")\n",
    "    plt.plot(range(19, len(episode_rewards)), mov_avg, label=\"20-ep moving avg\")\n",
    "plt.title(\"Reward Curve\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Optimization step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98afed",
   "metadata": {},
   "source": [
    "## 9) Evaluate the Trained Agent\n",
    "Run deterministic episodes and report mean return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(environment, episodes: int = 10):\n",
    "    returns = []\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env_reset(environment, seed=1000 + ep)\n",
    "        done = False\n",
    "        ep_return = 0.0\n",
    "        while not done:\n",
    "            action = select_action(state, epsilon=0.0)\n",
    "            state, reward, done, _ = env_step(environment, action)\n",
    "            ep_return += reward\n",
    "        returns.append(ep_return)\n",
    "    return returns\n",
    "\n",
    "\n",
    "eval_returns = evaluate_agent(env, episodes=10)\n",
    "print(\"Evaluation returns:\", eval_returns)\n",
    "print(\"Mean evaluation return:\", float(np.mean(eval_returns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d57e4c",
   "metadata": {},
   "source": [
    "## 10) Save, Load, and Reuse the Model\n",
    "Persist checkpoints, reload them, and run a quick inference episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"dqn_cartpole_checkpoint.pt\"\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        \"policy_net\": policy_net.state_dict(),\n",
    "        \"target_net\": target_net.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"config\": {\n",
    "            \"env_name\": ENV_NAME,\n",
    "            \"state_dim\": state_dim,\n",
    "            \"action_dim\": action_dim,\n",
    "            \"lr\": LR,\n",
    "            \"gamma\": GAMMA,\n",
    "        },\n",
    "    },\n",
    "    checkpoint_path,\n",
    ")\n",
    "print(\"Saved checkpoint:\", checkpoint_path)\n",
    "\n",
    "# PyTorch 2.6+ defaults torch.load(..., weights_only=True), which can reject\n",
    "# non-tensor metadata in checkpoints. This file is created in this notebook,\n",
    "# so using weights_only=False is safe here.\n",
    "loaded = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "policy_net.load_state_dict(loaded[\"policy_net\"])\n",
    "target_net.load_state_dict(loaded[\"target_net\"])\n",
    "optimizer.load_state_dict(loaded[\"optimizer\"])\n",
    "\n",
    "state, _ = env_reset(env, seed=2026)\n",
    "done = False\n",
    "rollout_return = 0.0\n",
    "while not done:\n",
    "    action = select_action(state, epsilon=0.0)\n",
    "    state, reward, done, _ = env_step(env, action)\n",
    "    rollout_return += reward\n",
    "\n",
    "print(\"Post-load deterministic rollout return:\", rollout_return)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
